{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ebcba45-3d27-47d5-ac58-14c890218846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as pl\n",
    "from scipy import stats, linalg\n",
    "\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "from thresholding import *\n",
    "import kneeliverse.rdp as rdp\n",
    "import kneeliverse.kneedle as kneedle\n",
    "\n",
    "#from castle.common import independence_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "596b19b3-28ae-4477-a9a8-0628e88795fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_corr(C):\n",
    "    \"\"\"\n",
    "    Returns the sample linear partial correlation coefficients between pairs of variables in C, controlling \n",
    "    for the remaining variables in C.\n",
    "    Parameters\n",
    "    ----------\n",
    "    C : array-like, shape (n, p)\n",
    "        Array with the different variables. Each column of C is taken as a variable\n",
    "    Returns\n",
    "    -------\n",
    "    P : array-like, shape (p, p)\n",
    "        P[i, j] contains the partial correlation of C[:, i] and C[:, j] controlling\n",
    "        for the remaining variables in C.\n",
    "    \"\"\"\n",
    "    \n",
    "    C = np.asarray(C)\n",
    "    p = C.shape[1]\n",
    "    P_corr = np.zeros((p, p))\n",
    "    for i in range(p):\n",
    "        P_corr[i, i] = 1\n",
    "        for j in range(i+1, p):\n",
    "            idx = np.ones(p, dtype=bool)\n",
    "            idx[i] = False\n",
    "            idx[j] = False\n",
    "            beta_i = linalg.lstsq(C[:, idx], C[:, j])[0]\n",
    "            beta_j = linalg.lstsq(C[:, idx], C[:, i])[0]\n",
    "\n",
    "            res_j = C[:, j] - C[:, idx].dot( beta_i)\n",
    "            res_i = C[:, i] - C[:, idx].dot(beta_j)\n",
    "            \n",
    "            corr = stats.pearsonr(res_i, res_j)[0]\n",
    "            P_corr[i, j] = corr\n",
    "            P_corr[j, i] = corr\n",
    "        \n",
    "    return P_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62603636-cdfb-429c-8cbb-8114b77bf665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total kept: 0.83\n",
      "Total kept: 0.82\n",
      "Total kept: 0.61\n",
      "Total kept: 0.5\n"
     ]
    }
   ],
   "source": [
    "#ETL\n",
    "df = pd.read_csv(\"notas-alunos-2012-2022-anonimo.csv\")\n",
    "total_len=len(df)\n",
    "\n",
    "df.drop(df[df.in_mobility==1].index,axis=0,inplace=True)\n",
    "df.drop([\"in_mobility\",\"mobility_start\",\"mobility_duration\"],axis=1,inplace=True)\n",
    "df.drop(df[(df.access_type == \"Other_8\")].index,axis=0,inplace=True)\n",
    "\n",
    "df = df[df.regime_type == \"Normal\"] #84%\n",
    "df.drop(\"regime_type\",axis=1,inplace=True)\n",
    "print(\"Total kept:\", np.round(len(df)/total_len,2) )\n",
    "\n",
    "df.drop(df[df.exam_type==\"December\"].index,axis=0,inplace=True)\n",
    "df.drop(df[df.exam_type==\"Semestral\"].index,axis=0,inplace=True)\n",
    "df.drop(df[df.exam_type==\"Special\"].index,axis=0,inplace=True)\n",
    "df.drop(df[df.exam_season==\"Special\"].index,axis=0,inplace=True)\n",
    "\n",
    "print(\"Total kept:\", np.round(len(df)/total_len,2) )\n",
    "\n",
    "df['istudent'] = df['istudent'].astype(str)\n",
    "\n",
    "df.drop(df[df.was_evaluated==0].index,axis=0,inplace=True) #non-evaluated\n",
    "df.drop(df[df.grade==0].index,axis=0,inplace=True) #removing zeros\n",
    "\n",
    "print(\"Total kept:\", np.round(len(df)/total_len,2) )\n",
    "\n",
    "df.drop(df[df.exam_season == \"Retake\"].index,axis=0,inplace=True) #recursos\n",
    "\n",
    "print(\"Total kept:\", np.round(len(df)/total_len,2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74331103-f08d-436a-a114-8e7fd45e52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram\n",
    "def histogramer(curso,col1,col2,i_,save):\n",
    "\n",
    "    x,y = valid_vals(X, col1, col2)[0].T\n",
    "    pl.plot(x,y,'.',alpha=0.4,label=\"Points: %i\"%len(x))\n",
    "    m,d,r2=stats.linregress(x,y)[:3]\n",
    "    pl.plot(np.array([0,20]),np.array([0,20])*m+d,'--',label=\"Fit: %.2f\"%r2)\n",
    "    \n",
    "    pl.title(\"Degree \"+str(curso))\n",
    "    pl.xlabel(\"Curricular Unit \"+str(col1))\n",
    "    pl.ylabel(\"Curricular Unit \"+str(col2))\n",
    "    pl.grid()\n",
    "    pl.ylim(0-0.5,20+0.5)\n",
    "    pl.xlim(0-0.5,20+0.5)\n",
    "    pl.legend()\n",
    "\n",
    "    if save:\n",
    "        pl.savefig(\"figures/neg_\"+str(i_))\n",
    "    pl.show()\n",
    "\n",
    "def valid_vals(X, col1, col2):\n",
    "    vals = []\n",
    "    studs= []\n",
    "    Xpair = (lambda dd: dd[dd.notna().all(axis=1)] )(X[[col1,col2]])\n",
    "    for p in Xpair.index:\n",
    "        d1,d2=Xpair.loc[p]\n",
    "                    \n",
    "        # Use efficient search for minimum positive difference\n",
    "        diffs = np.array(list(it.product(d1.keys(), d2.keys())))\n",
    "        pos_diffs = diffs[:, 1] - diffs[:, 0]\n",
    "        mask = pos_diffs > 0\n",
    "\n",
    "        if not np.any(mask):  # Skip if no positive differences exist\n",
    "            continue\n",
    "        \n",
    "        best_idx = np.argmin(pos_diffs[mask])\n",
    "        best_a, best_b = diffs[mask][best_idx]\n",
    "        # Append the corresponding values to vals\n",
    "        vals.append([d1[best_a], d2[best_b]])\n",
    "        studs.append(p)\n",
    "\n",
    "    return np.array(vals),np.array(studs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ce4b0ee-cacf-4d63-966c-e383c65d8b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "\n",
    "anticorr_total = []\n",
    "anticorr_signi = []\n",
    "\n",
    "i_ = 0\n",
    "for curso in np.unique(df.idegree):\n",
    "    dg = df[df.idegree==curso].copy()\n",
    "\n",
    "    B = nx.Graph()\n",
    "    B.add_nodes_from(dg['icourse'].unique(),bipartite=0)\n",
    "    B.add_nodes_from(dg['istudent'].unique(), bipartite=1)\n",
    "    B.add_edges_from(dg[['istudent','icourse']].values.tolist())\n",
    "    \n",
    "    G = nx.bipartite.weighted_projected_graph(B,dg['icourse'].unique())\n",
    "\n",
    "    n_weights = [ (dg.icourse == disc).sum()*0.7 for disc in G.nodes]\n",
    "\n",
    "    edges, weights = zip(*nx.get_edge_attributes(G, 'weight').items())\n",
    "    e_weights = [w * 0.1 for w in weights]\n",
    "\n",
    "    if save:\n",
    "        dg.nota.hist(bins=np.arange(21),color='skyblue',edgecolor='black')\n",
    "        pl.xticks(np.arange(21))\n",
    "        pl.xlim(0,20)\n",
    "        #pl.ylim(0,100000)\n",
    "        pl.grid(axis=\"x\")\n",
    "        pl.title(\"Distribution of scores in evaluated subjects\")\n",
    "        pl.xlabel(\"Scores\")\n",
    "        pl.savefig(\"figures/\"+str(curso)+\"_hist_scores\")\n",
    "        pl.show()\n",
    "    \n",
    "\n",
    "    cut_stud = 20\n",
    "    H = G.edge_subgraph([(a,b) for a, b, attrs in G.edges(data=True) if attrs[\"weight\"] >= cut_stud]).copy()\n",
    "    \n",
    "    comp_list = sorted(nx.connected_components(H), key=len, reverse=True)\n",
    "    \n",
    "    #LCC\n",
    "    if len(H.nodes()) == 0:\n",
    "        continue\n",
    "    H = H.subgraph(list(nx.connected_components(H))[0])\n",
    "    \n",
    "    n_weights = [ (dg.icourse == disc).sum()*0.5 for disc in H.nodes]\n",
    "    \n",
    "    edges, weights = zip(*nx.get_edge_attributes(H, 'weight').items())\n",
    "    e_weights = [w * 0.005 for w in weights]\n",
    "    \n",
    "    if save:\n",
    "        pl.figure(figsize=(16,16))\n",
    "        nx.draw(H, node_size=n_weights, width=e_weights, edgecolors='black')\n",
    "        pl.savefig(\"figures/\"+str(curso)+\"_network_cutoff20_LCC\")\n",
    "        pl.show()\n",
    "\n",
    "    # Initialize shared memory for the results\n",
    "    course_semester = mp.Manager().dict()\n",
    "    \n",
    "    # Define function to process each discipline\n",
    "    def process_disc(disc):\n",
    "        temp_ = np.sort(dg[dg.icourse == disc].exam_season.unique())\n",
    "        \n",
    "        # Define rules and add results to the dictionary\n",
    "        if temp_[0] == \"Annual\":\n",
    "            return disc, 2, None  # Second semester for annual course\n",
    "       # elif temp_[0] == \"Retake\":\n",
    "       #     return disc, None, None  # Ignore 'Retake' courses\n",
    "        elif \"1st Semester\" in temp_ and \"2nd Semester\" in temp_:\n",
    "            dg.loc[(dg.icourse == disc) & (dg.exam_season == \"2nd Semester\"), \"icourse\"] = -disc\n",
    "            return -disc, 2, disc  # New discipline with negative numbering\n",
    "        \n",
    "        return disc, int(temp_[0][0]), None  # Set semester based on first character of temp_\n",
    "    \n",
    "    # Parallel execution\n",
    "    with mp.Pool() as pool:\n",
    "        results = pool.map(process_disc, dg.icourse.unique())\n",
    "    \n",
    "    # Update course_semester and dg with the results\n",
    "    for disc, semester, updated_disc in results:\n",
    "        if semester is not None:\n",
    "            course_semester[disc] = semester\n",
    "        if updated_disc is not None:\n",
    "            course_semester[updated_disc] = 2  # Mark as second semester\n",
    "    \n",
    "    course_semester = dict(course_semester)\n",
    "\n",
    "    entry_year = dg.groupby('istudent')['year'].min().to_dict()\n",
    "\n",
    "\n",
    "    # Function to process each student's records\n",
    "    def process_student(student_group):\n",
    "        student, group = student_group\n",
    "        entry_year_student = entry_year[student]\n",
    "        \n",
    "        # Initialize courses dictionary for this student\n",
    "        courses = {col: None for col in course_semester.keys()}\n",
    "    \n",
    "        # Iterate over records for this student\n",
    "        for _, row in group.iterrows():\n",
    "            disc = row['icourse']\n",
    "            \n",
    "            if disc not in courses:\n",
    "                continue\n",
    "            \n",
    "            # Calculate semester\n",
    "            semt = (row['year'] - entry_year_student) * 2 + (course_semester[disc] - 1)\n",
    "            \n",
    "            current_grade = row['grade']\n",
    "            \n",
    "            # Update grades only if valid and lower than the stored grade\n",
    "            if courses[disc] is not None:\n",
    "                if semt in courses[disc]:\n",
    "                    if courses[disc][semt] > current_grade > 0:\n",
    "                        courses[disc][semt] = current_grade\n",
    "                elif current_grade > 0:\n",
    "                    courses[disc][semt] = current_grade\n",
    "            elif current_grade > 0:\n",
    "                courses[disc] = {semt: current_grade}\n",
    "                \n",
    "        return student, courses\n",
    "    \n",
    "    # Creating the DataFrame `X` in parallel\n",
    "    def create_X_dataframe(df):\n",
    "        # Initialize an empty DataFrame with the required shape and columns\n",
    "        X = pd.DataFrame(index=df['istudent'].unique(), columns=course_semester.keys())\n",
    "        \n",
    "        # Split data by 'istudent'\n",
    "        student_groups = list(df.groupby('istudent'))\n",
    "        \n",
    "        # Use multiprocessing Pool to process each student in parallel\n",
    "        with mp.Pool(mp.cpu_count()) as pool:\n",
    "            results = pool.map(process_student, student_groups)\n",
    "        \n",
    "        # Populate the DataFrame `X` with results from the parallel processing\n",
    "        for student, courses in results:\n",
    "            X.loc[student] = courses\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    # Call function to create the DataFrame\n",
    "    X = create_X_dataframe(dg)\n",
    "    X = X.dropna(axis=\"columns\",how=\"all\")\n",
    "    X = X.dropna(axis=\"index\",how=\"all\")\n",
    "\n",
    "\n",
    "    \n",
    "    ti = time.time()\n",
    "    \n",
    "    corr = {}\n",
    "    n_stud = {}\n",
    "    \n",
    "    valid_pairs = set(H.edges)\n",
    "    for col1,col2 in it.permutations(X.columns,2):\n",
    "    \n",
    "        #Skip if edge is not in H\n",
    "        if (abs(col1), abs(col2)) not in valid_pairs and (abs(col2), abs(col1)) not in valid_pairs:\n",
    "            continue\n",
    "    \n",
    "        vals=valid_vals(X, col1, col2)[0]\n",
    "            \n",
    "        if len(vals)<max(2,cut_stud): #cut-off min alunos #Note that it might leave the graph disconnected\n",
    "            continue\n",
    "    \n",
    "        if np.any(np.var(vals,axis=0)==0):\n",
    "            continue\n",
    "        \n",
    "        corr[(col1, col2)] = np.corrcoef(vals[:, 0], vals[:, 1])[0, 1]\n",
    "        #corr[(col1,col2)] = independence_tests.CITest.fisherz_test(vals,0,1,[])[2]\n",
    "        n_stud[(col1, col2)] = len(vals)\n",
    "\n",
    "    corr_vals = np.abs(list(corr.values()))\n",
    "    corr_vars = np.array(list(corr))\n",
    "    \n",
    "    corr_vals,corr_vars = np.sort(corr_vals), corr_vars[np.argsort(corr_vals)]\n",
    "    #corr_vars, corr_vals = np.flip(corr_vars,axis=0), np.flip(corr_vals,axis=0)\n",
    "    \n",
    "    # Initialize the graph with all edges\n",
    "    K_ = nx.Graph()\n",
    "    K_.add_edges_from(corr_vars)\n",
    "    # Create a subgraph of the largest connected component\n",
    "    if len(K_.nodes())==0:\n",
    "        continue\n",
    "    K = K_.subgraph(sorted(nx.connected_components(K_), key=len, reverse=True)[0]).copy()\n",
    "    corr_vars_ = corr_vars[np.array([i for i in range(len(corr_vars)) if corr_vars[i] in K.edges])]\n",
    "    corr_vals_ = corr_vals[np.array([i for i in range(len(corr_vars)) if corr_vars[i] in K.edges])]\n",
    "    \n",
    "    #Connected Threshold\n",
    "    m = binary_search(np.unique(corr_vars_),corr_vars_)\n",
    "    m1=m\n",
    "    \n",
    "    #'''\n",
    "    #Knee Threshold\n",
    "    gcc_nodes=np.zeros(len(corr_vars_))\n",
    "    for j in range(len(corr_vars_)-1):\n",
    "        # Get the size of the largest connected component\n",
    "        gcc_nodes[j] = len(max(nx.connected_components(K),key=len))\n",
    "    \n",
    "        jT = np.where(np.all(np.flip(corr_vars_[j]) == corr_vars_,axis=1))[0]\n",
    "        if len(jT)>0 and corr_vals_[j]>corr_vals_[jT]:\n",
    "            continue\n",
    "        \n",
    "        # Remove the edge corresponding to the current step\n",
    "        K.remove_edge(*corr_vars_[j])\n",
    "        \n",
    "    m = kneedle.knee(np.column_stack((np.arange(len(gcc_nodes)),gcc_nodes)))\n",
    "    thres = corr_vals[m]\n",
    "    \n",
    "    if not m:\n",
    "        continue\n",
    "    #'''\n",
    "    \n",
    "    \n",
    "    if save:\n",
    "        pl.plot(gcc_nodes,'.')\n",
    "        pl.plot(m1,gcc_nodes[m1],'*')\n",
    "        pl.plot(m,gcc_nodes[m],'r*')\n",
    "        pl.grid()\n",
    "        pl.savefig(\"figures/\"+str(curso)+\"_kneegraph\")\n",
    "        pl.show()\n",
    "\n",
    "    if save:\n",
    "        pl.hist(corr.values(),bins=np.linspace(-1,1,17),color='skyblue',edgecolor='black')\n",
    "        pl.title(\"Distribution of correlation coeficients\")\n",
    "        pl.xlim(-1,1)\n",
    "        ylims = pl.gca().get_ylim()\n",
    "        pl.plot(np.array([-thres,-thres]),ylims,'r--',label=\"Knee\")\n",
    "        pl.plot(np.array([thres,thres]),ylims,'r--')\n",
    "        pl.plot(np.array([-corr_vals[m1],-corr_vals[m1]]),ylims,'g--',label=\"Connected\")\n",
    "        pl.plot(np.array([corr_vals[m1],corr_vals[m1]]),ylims,'g--')\n",
    "        pl.grid(axis=\"y\")\n",
    "        pl.legend(loc=2)\n",
    "        pl.savefig(\"figures/\"+str(curso)+\"_corrhist\")\n",
    "        pl.show()\n",
    "\n",
    "    ti = time.time()\n",
    "    \n",
    "    G1 = nx.DiGraph()\n",
    "    G1.add_edges_from(corr_vars[m:])\n",
    "\n",
    "    G0 = nx.DiGraph()\n",
    "    G0.add_edges_from(corr_vars)\n",
    "\n",
    "    #Triangulation\n",
    "    '''\n",
    "    removed=[]\n",
    "    \n",
    "    for col1 in np.array(G1.nodes)[np.array(G1.in_degree)[:,1].astype(\"int\")>1]: #anchoer nodes with in-degree greater than 1\n",
    "        \n",
    "        parents = np.array(list(G1.in_edges(col1)))[:,0]\n",
    "    \n",
    "        for col2,col3 in it.permutations(parents,2):\n",
    "            \n",
    "            if (col2,col3) not in G.edges or (col3,col2) not in G.edges:\n",
    "                continue\n",
    "            \n",
    "            vals = []\n",
    "            for d1,d2,d3 in (lambda dd: dd[dd.notna().all(axis=1)] )(X[[col1,col2,col3]]).values:\n",
    "    \n",
    "                # Use efficient search for minimum positive difference\n",
    "                diffs12 = np.array(list(it.product(d1.keys(), d2.keys())))\n",
    "                pos_diffs12 = diffs12[:, 1] - diffs12[:, 0]\n",
    "                mask12 = pos_diffs12 > 0\n",
    "    \n",
    "                diffs13 = np.array(list(it.product(d1.keys(), d3.keys())))\n",
    "                pos_diffs13 = diffs13[:, 1] - diffs13[:, 0]\n",
    "                mask13 = pos_diffs13 > 0\n",
    "    \n",
    "                if not np.any(mask12) or not np.any(mask13):  # Skip if no positive differences exist\n",
    "                    continue\n",
    "    \n",
    "                best_a, best_b = diffs12[mask12][np.argmin(pos_diffs12[mask12])]\n",
    "                best_d, best_c = diffs13[mask13][np.argmin(pos_diffs13[mask13])]\n",
    "    \n",
    "                if best_a != best_d: #noncoincident semesters: no common cause found\n",
    "                    continue\n",
    "    \n",
    "                # Append the corresponding values to vals\n",
    "                vals.append([d1[best_a], d2[best_b], d3[best_c]])\n",
    "            \n",
    "            if len(vals)<=cut_stud: #cut-off min alunos\n",
    "                continue\n",
    "                \n",
    "            vals = np.array(vals)\n",
    "            \n",
    "            if np.any(np.var(vals,axis=0)==0):\n",
    "                continue\n",
    "    \n",
    "            partial_corrs = partial_corr (vals)\n",
    "            if (col2,col1) in G1.edges and np.abs(partial_corrs[0,1]) < thres:\n",
    "                G1.remove_edge(col2,col1)\n",
    "                removed.append((col2,col1,col3))\n",
    "                \n",
    "            if (col3,col1) in G1.edges and np.abs(partial_corrs[0,2]) < thres:\n",
    "                G1.remove_edge(col3,col1)\n",
    "                removed.append((col3,col1,col2))\n",
    "    #'''\n",
    "        \n",
    "    dres = pd.DataFrame(corr_vars[:],columns=[\"Var1\",\"Var2\"])\n",
    "    dres[\"corr_val\"] = corr_vals[:]\n",
    "    dres[\"StudentBoth\"] = [n_stud[(dres.loc[i][\"Var1\"],dres.loc[i][\"Var2\"])] for i in dres.index]\n",
    "    \n",
    "    dict_semmode = {colx: stats.mode(sum([list(ii.keys())\n",
    "                                          for ii in X[colx][X[colx].notna()] ],[]))[0]\n",
    "                    for colx in X.columns}\n",
    "    dres[\"SemesterMode1\"] = [dict_semmode[dres[\"Var1\"].loc[i]] for i in dres.index]\n",
    "    dres[\"SemesterMode2\"] = [dict_semmode[dres[\"Var2\"].loc[i]] for i in dres.index]\n",
    "    \n",
    "    dict_semfirsttry = {colx: stats.mode([list(ii.keys())[0]\n",
    "                                          for ii in X[colx][X[colx].notna()] ])[0]\n",
    "                    for colx in X.columns}\n",
    "    dres[\"SemesterFirstTry1\"] = [dict_semfirsttry[dres[\"Var1\"].loc[i]] for i in dres.index]\n",
    "    dres[\"SemesterFirstTry2\"] = [dict_semfirsttry[dres[\"Var2\"].loc[i]] for i in dres.index]\n",
    "    \n",
    "    dict_semfirsttry_ave = {colx: np.mean([list(ii.keys())[0]\n",
    "                                          for ii in X[colx][X[colx].notna()] ])\n",
    "                    for colx in X.columns}\n",
    "    dres[\"SemesterFirstTry1_Mean\"] = [dict_semfirsttry_ave[dres[\"Var1\"].loc[i]] for i in dres.index]\n",
    "    dres[\"SemesterFirstTry2_Mean\"] = [dict_semfirsttry_ave[dres[\"Var2\"].loc[i]] for i in dres.index]\n",
    "    \n",
    "    \n",
    "    dict_semmean = {}\n",
    "    for colx in X.columns:\n",
    "        # Combine all keys from non-None dictionaries in the column\n",
    "        keys = sum([list(ii.keys()) for ii in X[colx][X[colx].notna()]], [])\n",
    "        \n",
    "        # Compute the mean if keys exist; otherwise, assign np.nan\n",
    "        if keys:\n",
    "            dict_semmean[colx] = np.mean(keys)\n",
    "        else:\n",
    "            dict_semmean[colx] = np.nan\n",
    "    dres[\"SemesterMean1\"] = [dict_semmean[dres[\"Var1\"].loc[i]] for i in dres.index]\n",
    "    dres[\"SemesterMean2\"] = [dict_semmean[dres[\"Var2\"].loc[i]] for i in dres.index]\n",
    "    \n",
    "    dict_mean = {colx: np.round(np.mean(sum([list(ii.values())\n",
    "                                             for ii in X[colx][X[colx].notna()] ],[])),2)\n",
    "                 for colx in X.columns}\n",
    "    dres[\"Mean1\"] = [dict_mean[dres[\"Var1\"].loc[i]] for i in dres.index]\n",
    "    dres[\"Mean2\"] = [dict_mean[dres[\"Var2\"].loc[i]] for i in dres.index]\n",
    "    \n",
    "    dict_course = {colx:list(np.unique(dg.idegree[dg.icourse==abs(colx)])) for colx in X.columns}\n",
    "    dres[\"Courses1\"] = [dict_course[dres[\"Var1\"].loc[i]] for i in dres.index]\n",
    "    dres[\"Courses2\"] = [dict_course[dres[\"Var2\"].loc[i]] for i in dres.index]\n",
    "    \n",
    "    dict_years = {colx:list(np.unique(dg.year[dg.icourse==abs(colx)])) for colx in X.columns}\n",
    "    dres[\"Years1\"] = [dict_years[dres[\"Var1\"].loc[i]] for i in dres.index]\n",
    "    dres[\"Years2\"] = [dict_years[dres[\"Var2\"].loc[i]] for i in dres.index]\n",
    "\n",
    "    temp1_,temp2_ = np.unique(dres[\"Var1\"],return_index=True)\n",
    "    order = {temp1_[i]:dres.loc[temp2_[i]].SemesterFirstTry1 for i in range(len(temp1_))}\n",
    "    \n",
    "    temp1_,temp2_ = np.unique(dres[\"Var2\"],return_index=True)\n",
    "    for i in range(len(temp1_)):\n",
    "        order[temp1_[i]] = dres.loc[temp2_[i]].SemesterFirstTry2\n",
    "\n",
    "    G_ = nx.DiGraph()\n",
    "    G_.add_edges_from(corr_vars[m:])\n",
    "\n",
    "    \n",
    "    node_list = np.array(list(order))[np.argsort(list(order.values()))]\n",
    "    \n",
    "    heat = np.zeros([len(node_list),len(node_list)])\n",
    "    for i in range(len(node_list)):\n",
    "        for j in range(len(node_list)):\n",
    "            if (node_list[i],node_list[j]) in corr:\n",
    "                \n",
    "                if np.abs(corr[(node_list[i],node_list[j])]) < thres: continue\n",
    "                    \n",
    "                #heat[i,j] = 1-corr[(node_list[i],node_list[j])]\n",
    "                heat[i,j] = corr[(node_list[i],node_list[j])]\n",
    "                #heat[j,i] = 1#heat[i,j]\n",
    "\n",
    "\n",
    "    if save:\n",
    "        fig, ax = pl.subplots(figsize=(16,20))\n",
    "        pl.imshow(heat, cmap='RdGy', interpolation='nearest',vmin=-1, vmax=1)\n",
    "        pl.colorbar(shrink=0.65)\n",
    "        \n",
    "        pl.plot(np.arange(len(node_list)+1)-0.5,\n",
    "                np.arange(len(node_list)+1)-0.5,'k--')\n",
    "        #pl.grid()\n",
    "        \n",
    "        ax.xaxis.tick_top()\n",
    "        pl.xticks(range(len(node_list)),[a if a in cod2name else a for a in node_list],rotation=90,size=10)\n",
    "        pl.yticks(range(len(node_list)),[a if a in cod2name else a for a in node_list],rotation=20,size=10)\n",
    "        \n",
    "        for i in np.cumsum(np.unique(list(order.values()),return_counts=True)[1])[:-1]:\n",
    "            pl.plot(np.arange(len(node_list)+1)-0.5,\n",
    "                    np.zeros(len(node_list)+1)+i-0.5,'k-')\n",
    "            pl.plot(np.zeros(len(node_list)+1)+i-0.5,\n",
    "                    np.arange(len(node_list)+1)-0.5,'k-')\n",
    "        \n",
    "        pl.savefig(\"figures/\"+str(curso)+\"_heatmap.png\",dpi=300)\n",
    "        pl.show()\n",
    "\n",
    "    T = nx.DiGraph()\n",
    "    for i in G0.nodes:\n",
    "        T.add_node(i,layer=order[i])\n",
    "    \n",
    "    T.add_edges_from(G1.edges())\n",
    "    \n",
    "\n",
    "    if save:\n",
    "        pl.figure(figsize=(16,16))\n",
    "        pos = nx.multipartite_layout(T, subset_key=\"layer\")\n",
    "        for node in pos: #introduce a shift in the x axis\n",
    "            pos[node] = pos[node] + np.array([np.random.random()*0.02-0.01,0])\n",
    "\n",
    "        color_map = [\"tab:red\" if (node in G0.nodes - G1.nodes) else \"tab:blue\" for node in T.nodes]\n",
    "        color_edg = [\"tab:red\" if corr[edge]<0 else \"k\" for edge in T.edges]\n",
    "        width = [ {(dres[\"Var1\"].loc[i],dres[\"Var2\"].loc[i]):dres[\"StudentBoth\"].loc[i]/300 for i in dres.index}[edge] for edge in G1.edges ]\n",
    "        nx.draw_networkx_nodes(T,pos=pos, node_color=color_map)\n",
    "        nx.draw_networkx_edges(T,pos=pos,alpha=np.abs(corr_vals[m:]),\n",
    "                               edge_color=color_edg,width=width,arrowsize=20) #width scalled 1/300\n",
    "        nx.draw_networkx_labels(T,pos=pos)\n",
    "        pl.savefig(\"figures/\"+str(curso)+\"_graph_sem.png\")\n",
    "        pl.box(False)\n",
    "        pl.show()\n",
    "\n",
    "    #print(\"total edges:\\t\",len(T.edges))\n",
    "    #print(\"doubles edges:\\t\",int(len([edge for edge in T.edges if np.flip(edge) in T.edges])/2))\n",
    "    \n",
    "    [edge for edge in T.edges if np.flip(edge) in T.edges]\n",
    "\n",
    "    anticorr_total.append( np.sum(np.array(list(corr.values()))<0) )\n",
    "    anticorr_signi.append( np.sum(heat<0) )\n",
    "\n",
    "    \n",
    "    if np.any(heat<0):\n",
    "        for pair in node_list[np.array(np.where(heat<0)).T]:\n",
    "            histogramer(curso,pair[0],pair[1],i_,save)\n",
    "            i_+=1\n",
    "\n",
    "anticorr_total = np.array(anticorr_total)\n",
    "anticorr_signi = np.array(anticorr_signi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bdac07-678d-4c79-902f-73842bd92366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30df24d-2bd7-41d8-b3bb-549b15f6f55a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
